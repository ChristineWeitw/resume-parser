{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXJWy1GJzrhk"
   },
   "source": [
    "# Generic Resume Parser\n",
    "\n",
    "**Goal:** Given a resume (PDF or Word), extract a structured JSON object:\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Setup & Requirements** – optional installs, environment configuration  \n",
    "2. **Project Structure & Settings** – skill list, file handling options  \n",
    "3. **Text Extraction** – robust loaders for PDF/DOCX/TXT  \n",
    "4. **Gemini Parser** – JSON‑only extraction with strict schema  \n",
    "5. **Fallback Parser** – rules‑based extraction (email, name, skills)  \n",
    "6. **Unified Pipeline** – `parse_resume(file_path)` chooses the best available method  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy8PRsmt0pIV"
   },
   "source": [
    "## 1) Setup & Requirements\n",
    "- Install dependencies (if not already available)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVWZOv836FXX"
   },
   "source": [
    "**Environment variable :**  \n",
    "Set api key as `GEMINI_API_KEY` in your local .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 (v3.11.9:de54cf5be3, Apr  2 2024, 07:12:50) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "Interpreter: /Users/christinewei/Documents/resume-parser/.venv/bin/python\n",
      "Gemini OK → Hello, friend!\n",
      "spaCy OK → ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "import spacy\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Interpreter:\", sys.executable)\n",
    "\n",
    "load_dotenv()  # loads .env from project root\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "assert GEMINI_API_KEY, \"Set GEMINI_API_KEY in your .env\"\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "print(\"Gemini OK →\", model.generate_content(\"Say hi in 3 words.\").text.strip())\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy OK →\", nlp.pipe_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQjY3MAo6z4i",
    "outputId": "ea2cd8a5-f5a5-4236-b8c2-972cd454d123"
   },
   "outputs": [],
   "source": [
    "## Uncoment the lines below to install necessary libraries if you use colab\n",
    "# !pip install c google-generativeai python-dotenv\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iOd_d1kK3xdc"
   },
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa3ASXiO5k6P"
   },
   "source": [
    "### 2) Project Structure & Settings\n",
    "\n",
    "We keep a **small default skill list** to ensure the fallback works out of the box.  Here we use the esco_skills.csv dataset from Kaggle: https://www.kaggle.com/datasets/thenoob69/esco-skills\n",
    "\n",
    "Optionally, you can **extend** it by pointing to your own CSV or TXT file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a0TjftJQ7RXu"
   },
   "outputs": [],
   "source": [
    "# Build a default/general skills list set for fallback skill parse matching check\n",
    "\n",
    "def build_skill_map_from_csv(csv_path='esco_skills.csv'):\n",
    "    \"\"\"\n",
    "    Loads skills from the Kaggle ESCO CSV and creates a mapping from any\n",
    "    skill variation (alternative or primary) to its canonical name.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): The path to the Kaggle skills CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping every possible skill alias (in lowercase)\n",
    "              to its canonical, original-cased skill name. Returns an empty\n",
    "              dict if the file cannot be read.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Successfully loaded {csv_path} with {len(df)} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {csv_path}.\")\n",
    "        print(\"Please download the Kaggle ESCO skills CSV and place it in the correct directory.\")\n",
    "        return {}\n",
    "\n",
    "    # Create the mapping dictionary\n",
    "    skill_map = {}\n",
    "\n",
    "    # Drop rows where the primary skill label is missing, as they are unusable\n",
    "    df.dropna(subset=['label_cleaned'], inplace=True)\n",
    "\n",
    "    # Replace NaN in 'altLabels' with an empty string to prevent errors\n",
    "    df['altLabels'].fillna('', inplace=True)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Get the canonical skill name (e.g., \"Manage Musical Staff\")\n",
    "        canonical_skill = row['label_cleaned'].strip()\n",
    "\n",
    "        # Map the lowercase version of the canonical skill to itself\n",
    "        skill_map[canonical_skill.lower()] = canonical_skill\n",
    "\n",
    "        # Process the alternative labels\n",
    "        alt_labels_str = row['altLabels']\n",
    "\n",
    "        # The altLabels are often a long string; we can split them if a clear delimiter exists\n",
    "        # or treat common phrases. For robustness, we will treat them as space-separated words\n",
    "        # and also add the full strings. Note: This part might need refinement based on the exact\n",
    "        # format of the altLabels string. \n",
    "        alt_labels_list = alt_labels_str.strip().split('\\n')\n",
    "\n",
    "        for alt_label in alt_labels_list:\n",
    "            alt_label = alt_label.strip()\n",
    "            if alt_label:\n",
    "                # Map the lowercase version of the alias to the canonical skill\n",
    "                skill_map[alt_label.lower()] = canonical_skill\n",
    "\n",
    "    print(f\"Built a skill map with {len(skill_map)} total variations.\")\n",
    "    return skill_map\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Make sure to provide the correct path to your downloaded CSV file.\n",
    "# SKILL_MAP = build_skill_map_from_csv('/content/skills.csv')\n",
    "# print(SKILL_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmdqdH2v85vK"
   },
   "source": [
    "### 3) Text Extraction\n",
    "\n",
    "Robust file loader that supports **PDF**, **DOCX**, and **TXT**.  \n",
    "- Prefers `PyMuPDF` (fast/accurate) if installed.  \n",
    "- DOCX via `python-docx`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ahBJBXZH63hu"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"Extracts text from a .docx file.\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(file_path):\n",
    "    \"\"\"\n",
    "    Detects the file type and uses the appropriate function to extract text.\n",
    "    Returns the extracted text as a string.\n",
    "    \"\"\"\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    if file_extension == '.pdf':\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_extension}\")\n",
    "        return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Make sure to provide the correct path to your downloaded sample PDF or Word file for testing\n",
    "# sample_file_path = '~/sample_resume7.docx'\n",
    "# if os.path.exists(sample_pdf_path):\n",
    "#     extracted_content = extract_text(sample_file_path)\n",
    "#     print(\"--- Extracted Content ---\")\n",
    "#     print(extracted_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPGjklRc-Mc6"
   },
   "source": [
    "### 4) Gemini Parser (LLM‑first)\n",
    "\n",
    "We ask Gemini to output **strict JSON** with exactly the keys: `name`, `email`, `skills`.\n",
    "If the model is unavailable or no key is set, we **skip** this path and let the unified pipeline use the fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Nc-hhp9DBGQ9"
   },
   "outputs": [],
   "source": [
    "def parse_resume_with_genai(resume_text):\n",
    "    \"\"\"\n",
    "    Sends resume text to the GenAI model and asks for structured data extraction.\n",
    "    \"\"\"\n",
    "    if not resume_text:\n",
    "        return None\n",
    "\n",
    "    # This prompt is the key to the success of the parser.\n",
    "    # It clearly defines the task, the input, and the desired output format.\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert resume parser. Your task is to analyze the provided resume text and extract the following information: the candidate's full name, their email address, and a list of their skills.\n",
    "\n",
    "    The output MUST be a valid JSON object with the following structure:\n",
    "    {{\n",
    "      \"name\": \"...\",\n",
    "      \"email\": \"...\",\n",
    "      \"skills\": [\"...\", \"...\", \"...\"]\n",
    "    }}\n",
    "\n",
    "    ** With skills extraction, Keep in mind to ensure you firstly include all the skills under skills section in the resume as how it's named, then check carefully for the rest parts of the resume to distinguish if there is more skills.** \n",
    "    Do not include any explanations, introductions, or additional text outside of the JSON object.\n",
    "\n",
    "    Here is the resume text:\n",
    "    ---\n",
    "    {resume_text}\n",
    "    ---\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "\n",
    "        # Clean the response to ensure it's valid JSON\n",
    "        json_response_text = response.text.strip().replace('```json', '').replace('```', '')\n",
    "\n",
    "        # Parse the JSON string into a Python dictionary\n",
    "        return json.loads(json_response_text)\n",
    "        # return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during GenAI parsing: {e}\")\n",
    "        print(f\"Raw response was: {response.text}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDNoMrAU_VP-"
   },
   "source": [
    "### 5) Fallback Parser (Regex + Heuristics)\n",
    "\n",
    "- **Email** via robust regex  \n",
    "- **Name** from early lines (2–4 words, title case, avoid section headers)  \n",
    "- **Skills** via intersection against a skill list (configurable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwg2nOtXCSSE"
   },
   "source": [
    "**Name Extraction:**\n",
    "\n",
    "Extracts candidate names using a tiered approach:\n",
    "\n",
    "1) spaCy NER , 2) regex near email, and 3) regex from top of text as fallback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9qPSnJaQcw8p"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print(\"Spacy model not found. Please run: !python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "def clean_extracted_name(name_text):\n",
    "  \"\"\"\n",
    "  Cleans the extracted name by taking only the first line.\n",
    "  This solves the issue of capturing a name plus a job title on the next line.\n",
    "  \"\"\"\n",
    "  if name_text is None:\n",
    "      return None\n",
    "\n",
    "  # Split the text by newline characters and take the first element\n",
    "  return name_text.splitlines()[0].strip()\n",
    "\n",
    "def extract_name_with_ner(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Uses spaCy's Named Entity Recognition (NER) to find person names.\n",
    "    This is the most reliable method.\n",
    "    \"\"\"\n",
    "    if not nlp_model:\n",
    "        return None\n",
    "\n",
    "    doc = nlp_model(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            # Check if the name seems plausible (e.g., more than one word)\n",
    "            if len(ent.text.strip().split()) >= 2:\n",
    "                return clean_extracted_name(ent.text)\n",
    "    return None\n",
    "\n",
    "def extract_name_near_email(text, email):\n",
    "    \"\"\"\n",
    "    Finds a name-like pattern on the same line as or line above the email.\n",
    "    \"\"\"\n",
    "    if not email:\n",
    "        return None\n",
    "\n",
    "    # A more flexible regex for names, allowing for initials and hyphens\n",
    "    name_regex = r\"([A-Z][a-z'-]+(?:\\s+[A-Z][a-z'-]+|\\s+[A-Z]\\.)+)\"\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if email in line:\n",
    "            # 1. Check the same line as the email\n",
    "            match = re.search(name_regex, line)\n",
    "            if match:\n",
    "                # Ensure we didn't just match part of an email or URL\n",
    "                if '@' not in match.group(0):\n",
    "                    return clean_extracted_name(match.group(0))\n",
    "\n",
    "            # 2. If not found, check the line directly above (if it exists)\n",
    "            if i > 0:\n",
    "                match = re.search(name_regex, lines[i-1])\n",
    "                if match:\n",
    "                    return clean_extracted_name(match.group(0))\n",
    "    return None\n",
    "\n",
    "def extract_name_from_top(text):\n",
    "    \"\"\"\n",
    "    Cleans the top of the resume and uses regex to find the most likely name.\n",
    "    This is the final fallback method.\n",
    "    \"\"\"\n",
    "    # Look at the first 300 characters\n",
    "    text_top = text[:300]\n",
    "\n",
    "    # More flexible regex allowing for middle initials, different spacing, etc.\n",
    "    name_regex = r\"([A-Z][a-z'-]+(?:\\s+[A-Z][a-z'-]+|\\s+[A-Z]\\.)+)\"\n",
    "\n",
    "    match = re.search(name_regex, text_top)\n",
    "    if match:\n",
    "        return clean_extracted_name(match.group(0))\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdSmkCYhDsrP"
   },
   "source": [
    "Fallback Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3UMYSMSDdTVq"
   },
   "outputs": [],
   "source": [
    "def parse_resume_with_rules(resume_text, skill_map, nlp_model):\n",
    "    \"\"\"\n",
    "    Parses resume text using a multi-layered, rule-based approach.\n",
    "\n",
    "    Args:\n",
    "        resume_text (str): The raw text of the resume.\n",
    "        skill_map (dict): A mapping of skill variations to canonical names.\n",
    "        nlp_model: The loaded spaCy model for NER.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted information.\n",
    "    \"\"\"\n",
    "    if not resume_text:\n",
    "        return None\n",
    "\n",
    "    # --- Email Extraction (Do this first as it can help with name extraction) ---\n",
    "    email = None\n",
    "    email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', resume_text)\n",
    "    if email_match:\n",
    "        email = email_match.group(0)\n",
    "\n",
    "    # --- Name Extraction (Multi-layered approach) ---\n",
    "    name = None\n",
    "    # 1. Try NER first - it's the most intelligent\n",
    "    name = extract_name_with_ner(resume_text, nlp_model)\n",
    "\n",
    "    # 2. If NER fails, try finding the name near the email\n",
    "    if not name:\n",
    "        name = extract_name_near_email(resume_text, email)\n",
    "\n",
    "    # 3. As a last resort, use the improved top-of-document search\n",
    "    if not name:\n",
    "        name = extract_name_from_top(resume_text)\n",
    "\n",
    "    # --- Skill Extraction (Using the advanced SKILL_MAP) ---\n",
    "    found_canonical_skills = set()\n",
    "    resume_text_lower = resume_text.lower()\n",
    "    if skill_map:\n",
    "        for skill_alias, canonical_skill in skill_map.items():\n",
    "            if re.search(r'\\b' + re.escape(skill_alias) + r'\\b', resume_text_lower):\n",
    "                found_canonical_skills.add(canonical_skill)\n",
    "\n",
    "    # --- Assemble final JSON object ---\n",
    "    parsed_data = {\n",
    "        \"name\": name,\n",
    "        \"email\": email,\n",
    "        \"skills\": sorted(list(found_canonical_skills))\n",
    "    }\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "# --- FINAL EXECUTION EXAMPLE ---\n",
    "# 1. Load dependencies at the start\n",
    "# SKILL_MAP = build_skill_map_from_csv('skills.csv')\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5e9CArbD3rD"
   },
   "source": [
    "### 6) Unified Pipeline\n",
    "\n",
    "`parse_resume(file_path)` tries **Gemini** first (if available), otherwise falls back to the rules‑based parser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H0lErsX7Dd7d"
   },
   "outputs": [],
   "source": [
    "def process_resume(file_path, skill_map):\n",
    "    \"\"\"\n",
    "    Main function to process a resume file.\n",
    "    It extracts text, tries the GenAI parser, and uses a rule-based\n",
    "    fallback with an advanced skill map if the GenAI parser fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing {os.path.basename(file_path)} ---\")\n",
    "\n",
    "    text = extract_text(file_path)\n",
    "    if not text:\n",
    "        print(\"Could not extract text. Aborting.\")\n",
    "        return None, \"Error\"\n",
    "\n",
    "    print(\"Attempting to parse with GenAI...\")\n",
    "    parsed_data = parse_resume_with_genai(text)\n",
    "\n",
    "    if parsed_data is None:\n",
    "        print(\"GenAI parsing failed. Switching to rule-based fallback parser...\")\n",
    "        # Pass the loaded SKILL_MAP to the fallback function\n",
    "        parsed_data = parse_resume_with_rules(text, skill_map)\n",
    "        parser_used = \"Fallback (Rules)\"\n",
    "    else:\n",
    "        parser_used = \"Primary (GenAI)\"\n",
    "\n",
    "    print(f\"Parsing complete. Method used: {parser_used}\")\n",
    "    return parsed_data, parser_used\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "OUT_DIR = Path(\"outputs\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def process_resume(file_path, skill_map, use_llm: bool = True):\n",
    "    \"\"\"\n",
    "    Main function to process a resume file.\n",
    "    It extracts text, tries the GenAI parser, and uses a rule-based\n",
    "    fallback with an advanced skill map if the GenAI parser fails.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    # Example: detect file type robustly\n",
    "    ext = file_path.suffix.lower()\n",
    "    if ext == \".pdf\":\n",
    "        # if your PDF reader needs a str path, cast with str(file_path)\n",
    "        text = extract_text_from_pdf(str(file_path))\n",
    "    elif ext in {\".docx\", \".doc\"}:\n",
    "        text = extract_text_from_docx(str(file_path))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported resume format: {file_path}\")\n",
    "\n",
    "    text = extract_text(file_path)\n",
    "    if not text:\n",
    "        print(\"Could not extract text. Aborting.\")\n",
    "        return None, \"Error\"\n",
    "\n",
    "    print(\"Attempting to parse with GenAI...\")\n",
    "    parsed_data = parse_resume_with_genai(text)\n",
    "\n",
    "    if (parsed_data is None) or (use_llm==False):\n",
    "        print(\"GenAI parsing failed. Switching to rule-based fallback parser...\")\n",
    "        # Pass the loaded SKILL_MAP to the fallback function\n",
    "        parsed_data = parse_resume_with_rules(text, skill_map, nlp)\n",
    "        parser_used = \"Fallback (Rules)\"\n",
    "    else:\n",
    "        parser_used = \"Primary (GenAI)\"\n",
    "\n",
    "    print(f\"Parsing complete. Method used: {parser_used}\")\n",
    "    # save JSON results to outputs \n",
    "    with open(OUT_DIR / f\"{file_path.stem}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(parsed_data, f, ensure_ascii=False, indent=2)\n",
    "    return parsed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq3iaWXrEydz"
   },
   "source": [
    "### 7) Final Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded skills.csv with 13893 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/bc2q7h4x0xd_1v2gtlr1f7xc0000gp/T/ipykernel_38416/1537022553.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['altLabels'].fillna('', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a skill map with 27178 total variations.\n",
      "Attempting to parse with GenAI...\n",
      "Parsing complete. Method used: Primary (GenAI)\n",
      "Attempting to parse with GenAI...\n",
      "Parsing complete. Method used: Primary (GenAI)\n",
      "Attempting to parse with GenAI...\n",
      "Parsing complete. Method used: Primary (GenAI)\n",
      "Attempting to parse with GenAI...\n",
      "Parsing complete. Method used: Primary (GenAI)\n",
      "Could not extract text. Aborting.\n",
      "Attempting to parse with GenAI...\n",
      "Parsing complete. Method used: Primary (GenAI)\n",
      "Attempting to parse with GenAI...\n",
      "Parsing complete. Method used: Primary (GenAI)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " [{'name': 'Kristen Connelly',\n",
       "   'email': 'email@email.com',\n",
       "   'skills': ['Call Sheets & Sides',\n",
       "    'Adobe Premiere Pro',\n",
       "    'Camera Boom, Light Boom, Mic Boom',\n",
       "    'DaVinci Resolve',\n",
       "    'scriptwriting',\n",
       "    'audio editing',\n",
       "    'mixing',\n",
       "    'Video Production',\n",
       "    'graphics editing',\n",
       "    'videography proposals',\n",
       "    'Production Acquisition',\n",
       "    'storytelling',\n",
       "    'digital video editing']},\n",
       "  {'name': 'Mandy Campbell',\n",
       "   'email': 'email@email.com',\n",
       "   'skills': ['Cardio Training',\n",
       "    'Fitness Routines',\n",
       "    'HIIT',\n",
       "    'Client Assessments',\n",
       "    'Health & Safety',\n",
       "    'Active Listening',\n",
       "    'Personalized Fitness Assessments',\n",
       "    'Marketing',\n",
       "    'Staffing',\n",
       "    'Sales',\n",
       "    'High-intensity training',\n",
       "    'Strength and conditioning',\n",
       "    'Nutrition',\n",
       "    'Group Cycling',\n",
       "    'Personal Training',\n",
       "    'CrossFit Level 1 Instructor',\n",
       "    'Advanced First Aid',\n",
       "    'Social Media Marketing',\n",
       "    'Program Budgeting',\n",
       "    'Program Statistics Analysis',\n",
       "    'Client Rapport',\n",
       "    'Class Schedule Redesign']},\n",
       "  {'name': 'MICHELLE LOPEZ',\n",
       "   'email': 'email@email.com',\n",
       "   'skills': ['Adobe Illustrator',\n",
       "    'Hand Drafting',\n",
       "    'Fabric',\n",
       "    'Fashion Design',\n",
       "    'Design Patterns',\n",
       "    'Product Design',\n",
       "    'Market Research',\n",
       "    'Mood Board Development',\n",
       "    'Sketching',\n",
       "    'Technical Design',\n",
       "    'Trim and Fabric Selection',\n",
       "    'Vendor Management',\n",
       "    'Commercial Design',\n",
       "    'Customer Needs Prediction',\n",
       "    'Relationship Management',\n",
       "    'Preproduction',\n",
       "    'Merchandising',\n",
       "    'Product Development',\n",
       "    'Design Theory',\n",
       "    'CAD',\n",
       "    'Uniform Design']},\n",
       "  {'name': 'JULIE MONROE',\n",
       "   'email': 'email@email.com',\n",
       "   'skills': ['Food preparation',\n",
       "    'Kitchen maintenance',\n",
       "    'Kitchen equipment operation',\n",
       "    'Food sanitation',\n",
       "    'Nutrition',\n",
       "    'nutrition',\n",
       "    'interpersonal skills',\n",
       "    'nutrition education counseling',\n",
       "    'nutrition assessments',\n",
       "    'WIC certifications',\n",
       "    'behavior and lifestyle changes',\n",
       "    'osteodystrophy management',\n",
       "    'kinetics',\n",
       "    'anemia management',\n",
       "    'nutrition management',\n",
       "    'bone mineral status monitoring',\n",
       "    'anemia monitoring',\n",
       "    'nutrition educational support']},\n",
       "  (None, 'Error'),\n",
       "  {'name': 'Lisa Shaw',\n",
       "   'email': 'firstname@resumetemplate.org',\n",
       "   'skills': ['Business Development',\n",
       "    'Talent Acquisition',\n",
       "    'Recruitment',\n",
       "    'Talent Sourcing',\n",
       "    'Client Services',\n",
       "    'Candidate Facilitation',\n",
       "    'Strategic & Operational Management',\n",
       "    'Graduate Recruitment',\n",
       "    'Sales Strategy',\n",
       "    'Hiring',\n",
       "    'Training',\n",
       "    'Operational Systems',\n",
       "    'Sales',\n",
       "    'Candidate Qualification',\n",
       "    'Interview Coordination',\n",
       "    'Offer Process Management',\n",
       "    'Induction Process',\n",
       "    'Actuarial',\n",
       "    'Audit',\n",
       "    'Risk Management',\n",
       "    'Engineering & Manufacturing',\n",
       "    'Problem Analysis',\n",
       "    'Solution Implementation',\n",
       "    'Interpersonal Skills',\n",
       "    'Communication Skills']},\n",
       "  {'name': 'JACKY WILSON',\n",
       "   'email': 'example@resumeviking.com',\n",
       "   'skills': ['Proficient in CNC machining',\n",
       "    'OSHA Trained',\n",
       "    'Can work Lathes Machines',\n",
       "    'manual lathe',\n",
       "    'mill',\n",
       "    'grinder',\n",
       "    'band saw operation',\n",
       "    'Block squaring',\n",
       "    'Minor G code/M code editing',\n",
       "    'Operate CNC Haas VM6 (3 axes) machinery',\n",
       "    'in-process QC inspection',\n",
       "    'machine and shop maintenance',\n",
       "    'Utilize all methods necessary to make fixtures including hand-drawn, and computer-aided models',\n",
       "    'Maintain inventory of assorted products and order management',\n",
       "    'Operate a CNC machine to machine parts for turbine engines in Abram Tanks and Airplanes',\n",
       "    'Load parts in CNC machine with precision',\n",
       "    'Unload and measure the part for accuracy',\n",
       "    'Load and unload dry freight and materials and maintain the warehouse workflow']}])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "# 1. First, build the map\n",
    "SKILL_MAP = build_skill_map_from_csv('skills.csv')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "samples = sorted(glob(\"sample_resume*.pdf\")) + sorted(glob(\"sample_resume*.docx\"))\n",
    "results = [process_resume(p, SKILL_MAP, use_llm=True) for p in samples]\n",
    "len(results), results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping (cannot map to answer): outputs/sample_resume4.json\n",
      "Skipping (cannot map to answer): outputs/sample_resume2.json\n",
      "Skipping (cannot map to answer): outputs/sample_resume3.json\n",
      "Skipping (cannot map to answer): outputs/sample_resume1.json\n",
      "Skipping (cannot map to answer): outputs/sample_resume6.json\n",
      "Skipping (cannot map to answer): outputs/sample_resume7.json\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'file'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/6f/bc2q7h4x0xd_1v2gtlr1f7xc0000gp/T/ipykernel_38416/2343001298.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     62\u001b[39m         \u001b[33m\"skills_f1\"\u001b[39m: round(f1, \u001b[32m3\u001b[39m),\n\u001b[32m     63\u001b[39m         \u001b[33m\"used_llm\"\u001b[39m: pred.get(\u001b[33m\"used_llm\"\u001b[39m)\n\u001b[32m     64\u001b[39m     })\n\u001b[32m     65\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m df = pd.DataFrame(rows).sort_values(\u001b[33m\"file\"\u001b[39m)\n\u001b[32m     67\u001b[39m display(df)\n\u001b[32m     68\u001b[39m \n\u001b[32m     69\u001b[39m summary = pd.DataFrame({\n",
      "\u001b[32m~/Documents/resume-parser/.venv/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7192\u001b[39m             )\n\u001b[32m   7193\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7194\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7195\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7196\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7197\u001b[39m \n\u001b[32m   7198\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7199\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/Documents/resume-parser/.venv/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'file'"
     ]
    }
   ],
   "source": [
    "import json, glob, re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Normalizers (strip < >, trim, lowercase) ----------\n",
    "def _norm_text(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    # remove surrounding angle brackets and extra spaces\n",
    "    s = re.sub(r'^[<\\s]+|[>\\s]+$', '', s)\n",
    "    return s\n",
    "\n",
    "def _norm_name(s):  return _norm_text(s).lower()\n",
    "def _norm_email(s): return _norm_text(s).lower()\n",
    "\n",
    "def _norm_skills(lst):\n",
    "    if not lst:\n",
    "        return set()\n",
    "    return { _norm_text(x).lower() for x in lst if str(x).strip() }\n",
    "\n",
    "# ---------- Load ground truth from answer.jsonl ----------\n",
    "with open(\"answer.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    answer = [json.loads(l) for l in f if l.strip()]\n",
    "\n",
    "# make sure 'file' is a simple basename (e.g., sample_resume1.pdf)\n",
    "for g in answer:\n",
    "    g[\"file\"] = Path(g[\"file\"]).name\n",
    "\n",
    "answer_map  = { g[\"file\"]: g for g in answer }\n",
    "answer_keys = set(answer_map.keys())\n",
    "\n",
    "# ---------- Helper: map prediction to a ground-truth filename ----------\n",
    "def map_pred_to_answer_filename(output_path, pred_dict):\n",
    "    \"\"\"\n",
    "    1) Prefer 'file' inside prediction if present.\n",
    "    2) Otherwise infer from output filename stem and try .pdf/.docx against answer_keys.\n",
    "    \"\"\"\n",
    "    inner = pred_dict.get(\"file\")\n",
    "    if inner:\n",
    "        return Path(inner).name\n",
    "\n",
    "    stem = Path(output_path).stem  # e.g., 'sample_resume1'\n",
    "    for ext in (\".pdf\", \".docx\"):\n",
    "        cand = stem + ext\n",
    "        if cand in answer_keys:\n",
    "            return cand\n",
    "    return None  # couldn't map\n",
    "\n",
    "# ---------- Load predictions and compute metrics ----------\n",
    "rows = []\n",
    "\n",
    "# Ensure outputs exist; you should have run your parser first\n",
    "pred_paths = sorted(glob.glob(\"outputs/*.json\"))\n",
    "if not pred_paths:\n",
    "    print(\"No prediction files found in outputs/. Run your parser cells first.\")\n",
    "\n",
    "for p in pred_paths:\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        pred = json.load(f)\n",
    "\n",
    "    fn = map_pred_to_answer_filename(p, pred)\n",
    "    if not fn:\n",
    "        print(f\"Skipping (cannot map to answer): {p}\")\n",
    "        continue\n",
    "\n",
    "    g = answer_map.get(fn)\n",
    "    if not g:\n",
    "        print(f\"No answer entry for: {fn}\")\n",
    "        continue\n",
    "\n",
    "    # Field metrics\n",
    "    name_acc  = int(_norm_name(pred.get(\"name\"))  == _norm_name(g.get(\"name\")))\n",
    "    email_acc = int(_norm_email(pred.get(\"email\")) == _norm_email(g.get(\"email\")))\n",
    "\n",
    "    P, G = _norm_skills(pred.get(\"skills\")), _norm_skills(g.get(\"skills\"))\n",
    "    tp = len(P & G); fp = len(P - G); fn_miss = len(G - P)\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec  = tp / (tp + fn_miss) if (tp + fn_miss) else 0.0\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec) else 0.0\n",
    "\n",
    "    rows.append({\n",
    "        \"file\": fn,\n",
    "        \"name_acc\": name_acc,\n",
    "        \"email_acc\": email_acc,\n",
    "        \"skills_prec\": round(prec, 3),\n",
    "        \"skills_rec\": round(rec, 3),\n",
    "        \"skills_f1\": round(f1, 3),\n",
    "        \"used_llm\": pred.get(\"used_llm\", None)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"file\")\n",
    "display(df)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"metric\": [\"name_acc\",\"email_acc\",\"skills_prec\",\"skills_rec\",\"skills_f1\"],\n",
    "    \"mean\": [\n",
    "        df[\"name_acc\"].mean()  if not df.empty else 0.0,\n",
    "        df[\"email_acc\"].mean() if not df.empty else 0.0,\n",
    "        df[\"skills_prec\"].mean() if not df.empty else 0.0,\n",
    "        df[\"skills_rec\"].mean()  if not df.empty else 0.0,\n",
    "        df[\"skills_f1\"].mean()   if not df.empty else 0.0,\n",
    "    ]\n",
    "})\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Demonstrate fallback works (one sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to parse with GenAI...\n",
      "GenAI parsing failed. Switching to rule-based fallback parser...\n",
      "Parsing complete. Method used: Fallback (Rules)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': '• Contributed',\n",
       " 'email': 'email@email.com',\n",
       " 'skills': ['adobe illustrator',\n",
       "  'design process',\n",
       "  'english',\n",
       "  'history',\n",
       "  'italian',\n",
       "  'market research']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. First, build the map\n",
    "# SKILL_MAP = build_skill_map_from_csv('skills.csv')\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "fallback_sample = process_resume(\"sample_resume3.pdf\", SKILL_MAP, use_llm=False)\n",
    "# json.load(open(\"outputs/fallback_sample_resume4.json\"))\n",
    "fallback_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (resume_env)",
   "language": "python",
   "name": "resume_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
