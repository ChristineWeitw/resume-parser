{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXJWy1GJzrhk"
   },
   "source": [
    "# Generic Resume Parser\n",
    "\n",
    "**Goal:** Given a resume (PDF or Word), extract a structured JSON object:\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Setup & Requirements** – optional installs, environment configuration  \n",
    "2. **Project Structure & Settings** – skill list, file handling options  \n",
    "3. **Text Extraction** – robust loaders for PDF/DOCX/TXT  \n",
    "4. **Gemini Parser** – JSON‑only extraction with strict schema  \n",
    "5. **Fallback Parser** – rules‑based extraction (email, name, skills)  \n",
    "6. **Unified Pipeline** – `parse_resume(file_path)` chooses the best available method  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy8PRsmt0pIV"
   },
   "source": [
    "## 1) Setup & Requirements\n",
    "- Install dependencies (if not already available)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVWZOv836FXX"
   },
   "source": [
    "**Environment variable :**  \n",
    "Set api key as `GEMINI_API_KEY` in your local .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 (v3.11.9:de54cf5be3, Apr  2 2024, 07:12:50) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "Interpreter: /Users/christinewei/Documents/resume-parser/.venv/bin/python\n",
      "Gemini OK → Hello, friend!\n",
      "spaCy OK → ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "import spacy\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Interpreter:\", sys.executable)\n",
    "\n",
    "load_dotenv()  # loads .env from project root\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "assert GEMINI_API_KEY, \"Set GEMINI_API_KEY in your .env\"\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "print(\"Gemini OK →\", model.generate_content(\"Say hi in 3 words.\").text.strip())\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy OK →\", nlp.pipe_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQjY3MAo6z4i",
    "outputId": "ea2cd8a5-f5a5-4236-b8c2-972cd454d123"
   },
   "outputs": [],
   "source": [
    "## Uncoment the lines below to install necessary libraries if you use colab\n",
    "# !pip install c google-generativeai python-dotenv\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iOd_d1kK3xdc"
   },
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa3ASXiO5k6P"
   },
   "source": [
    "### 2) Project Structure & Settings\n",
    "\n",
    "We keep a **small default skill list** to ensure the fallback works out of the box.  Here we use the esco_skills.csv dataset from Kaggle: https://www.kaggle.com/datasets/thenoob69/esco-skills\n",
    "\n",
    "Optionally, you can **extend** it by pointing to your own CSV or TXT file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a0TjftJQ7RXu"
   },
   "outputs": [],
   "source": [
    "# Build a default/general skills list set for fallback skill parse matching check\n",
    "\n",
    "def build_skill_map_from_csv(csv_path='esco_skills.csv'):\n",
    "    \"\"\"\n",
    "    Loads skills from the Kaggle ESCO CSV and creates a mapping from any\n",
    "    skill variation (alternative or primary) to its canonical name.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): The path to the Kaggle skills CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping every possible skill alias (in lowercase)\n",
    "              to its canonical, original-cased skill name. Returns an empty\n",
    "              dict if the file cannot be read.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Successfully loaded {csv_path} with {len(df)} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {csv_path}.\")\n",
    "        print(\"Please download the Kaggle ESCO skills CSV and place it in the correct directory.\")\n",
    "        return {}\n",
    "\n",
    "    # Create the mapping dictionary\n",
    "    skill_map = {}\n",
    "\n",
    "    # Drop rows where the primary skill label is missing, as they are unusable\n",
    "    df.dropna(subset=['label_cleaned'], inplace=True)\n",
    "\n",
    "    # Replace NaN in 'altLabels' with an empty string to prevent errors\n",
    "    df['altLabels'].fillna('', inplace=True)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Get the canonical skill name (e.g., \"Manage Musical Staff\")\n",
    "        canonical_skill = row['label_cleaned'].strip()\n",
    "\n",
    "        # Map the lowercase version of the canonical skill to itself\n",
    "        skill_map[canonical_skill.lower()] = canonical_skill\n",
    "\n",
    "        # Process the alternative labels\n",
    "        alt_labels_str = row['altLabels']\n",
    "\n",
    "        # The altLabels are often a long string; we can split them if a clear delimiter exists\n",
    "        # or treat common phrases. For robustness, we will treat them as space-separated words\n",
    "        # and also add the full strings. Note: This part might need refinement based on the exact\n",
    "        # format of the altLabels string. \n",
    "        alt_labels_list = alt_labels_str.strip().split('\\n')\n",
    "\n",
    "        for alt_label in alt_labels_list:\n",
    "            alt_label = alt_label.strip()\n",
    "            if alt_label:\n",
    "                # Map the lowercase version of the alias to the canonical skill\n",
    "                skill_map[alt_label.lower()] = canonical_skill\n",
    "\n",
    "    print(f\"Built a skill map with {len(skill_map)} total variations.\")\n",
    "    return skill_map\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Make sure to provide the correct path to your downloaded CSV file.\n",
    "# SKILL_MAP = build_skill_map_from_csv('/content/skills.csv')\n",
    "# print(SKILL_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmdqdH2v85vK"
   },
   "source": [
    "### 3) Text Extraction\n",
    "\n",
    "Robust file loader that supports **PDF**, **DOCX**, and **TXT**.  \n",
    "- Prefers `PyMuPDF` (fast/accurate) if installed.  \n",
    "- DOCX via `python-docx`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ahBJBXZH63hu"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"Extracts text from a .docx file.\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(file_path):\n",
    "    \"\"\"\n",
    "    Detects the file type and uses the appropriate function to extract text.\n",
    "    Returns the extracted text as a string.\n",
    "    \"\"\"\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    if file_extension == '.pdf':\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_extension}\")\n",
    "        return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Make sure to provide the correct path to your downloaded sample PDF or Word file for testing\n",
    "# sample_file_path = '~/sample_resume7.docx'\n",
    "# if os.path.exists(sample_pdf_path):\n",
    "#     extracted_content = extract_text(sample_file_path)\n",
    "#     print(\"--- Extracted Content ---\")\n",
    "#     print(extracted_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPGjklRc-Mc6"
   },
   "source": [
    "### 4) Gemini Parser (LLM‑first)\n",
    "\n",
    "We ask Gemini to output **strict JSON** with exactly the keys: `name`, `email`, `skills`.\n",
    "If the model is unavailable or no key is set, we **skip** this path and let the unified pipeline use the fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Nc-hhp9DBGQ9"
   },
   "outputs": [],
   "source": [
    "def parse_resume_with_genai(resume_text):\n",
    "    \"\"\"\n",
    "    Sends resume text to the GenAI model and asks for structured data extraction.\n",
    "    \"\"\"\n",
    "    if not resume_text:\n",
    "        return None\n",
    "\n",
    "    # This prompt is the key to the success of the parser.\n",
    "    # It clearly defines the task, the input, and the desired output format.\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert resume parser. Your task is to analyze the provided resume text and extract the following information: the candidate's full name, their email address, and a list of their skills.\n",
    "\n",
    "    The output MUST be a valid JSON object with the following structure:\n",
    "    {{\n",
    "      \"name\": \"...\",\n",
    "      \"email\": \"...\",\n",
    "      \"skills\": [\"...\", \"...\", \"...\"]\n",
    "    }}\n",
    "\n",
    "    Do not include any explanations, introductions, or additional text outside of the JSON object.\n",
    "\n",
    "    Here is the resume text:\n",
    "    ---\n",
    "    {resume_text}\n",
    "    ---\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "\n",
    "        # Clean the response to ensure it's valid JSON\n",
    "        json_response_text = response.text.strip().replace('```json', '').replace('```', '')\n",
    "\n",
    "        # Parse the JSON string into a Python dictionary\n",
    "        return json.loads(json_response_text)\n",
    "        # return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during GenAI parsing: {e}\")\n",
    "        print(f\"Raw response was: {response.text}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDNoMrAU_VP-"
   },
   "source": [
    "### 5) Fallback Parser (Regex + Heuristics)\n",
    "\n",
    "- **Email** via robust regex  \n",
    "- **Name** from early lines (2–4 words, title case, avoid section headers)  \n",
    "- **Skills** via intersection against a skill list (configurable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwg2nOtXCSSE"
   },
   "source": [
    "**Name Extraction:**\n",
    "\n",
    "Extracts candidate names using a tiered approach:\n",
    "\n",
    "1) spaCy NER , 2) regex near email, and 3) regex from top of text as fallback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9qPSnJaQcw8p"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print(\"Spacy model not found. Please run: !python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "def clean_extracted_name(name_text):\n",
    "  \"\"\"\n",
    "  Cleans the extracted name by taking only the first line.\n",
    "  This solves the issue of capturing a name plus a job title on the next line.\n",
    "  \"\"\"\n",
    "  if name_text is None:\n",
    "      return None\n",
    "\n",
    "  # Split the text by newline characters and take the first element\n",
    "  return name_text.splitlines()[0].strip()\n",
    "\n",
    "def extract_name_with_ner(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Uses spaCy's Named Entity Recognition (NER) to find person names.\n",
    "    This is the most reliable method.\n",
    "    \"\"\"\n",
    "    if not nlp_model:\n",
    "        return None\n",
    "\n",
    "    doc = nlp_model(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            # Check if the name seems plausible (e.g., more than one word)\n",
    "            if len(ent.text.strip().split()) >= 2:\n",
    "                return clean_extracted_name(ent.text)\n",
    "    return None\n",
    "\n",
    "def extract_name_near_email(text, email):\n",
    "    \"\"\"\n",
    "    Finds a name-like pattern on the same line as or line above the email.\n",
    "    \"\"\"\n",
    "    if not email:\n",
    "        return None\n",
    "\n",
    "    # A more flexible regex for names, allowing for initials and hyphens\n",
    "    name_regex = r\"([A-Z][a-z'-]+(?:\\s+[A-Z][a-z'-]+|\\s+[A-Z]\\.)+)\"\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if email in line:\n",
    "            # 1. Check the same line as the email\n",
    "            match = re.search(name_regex, line)\n",
    "            if match:\n",
    "                # Ensure we didn't just match part of an email or URL\n",
    "                if '@' not in match.group(0):\n",
    "                    return clean_extracted_name(match.group(0))\n",
    "\n",
    "            # 2. If not found, check the line directly above (if it exists)\n",
    "            if i > 0:\n",
    "                match = re.search(name_regex, lines[i-1])\n",
    "                if match:\n",
    "                    return clean_extracted_name(match.group(0))\n",
    "    return None\n",
    "\n",
    "def extract_name_from_top(text):\n",
    "    \"\"\"\n",
    "    Cleans the top of the resume and uses regex to find the most likely name.\n",
    "    This is the final fallback method.\n",
    "    \"\"\"\n",
    "    # Look at the first 300 characters\n",
    "    text_top = text[:300]\n",
    "\n",
    "    # More flexible regex allowing for middle initials, different spacing, etc.\n",
    "    name_regex = r\"([A-Z][a-z'-]+(?:\\s+[A-Z][a-z'-]+|\\s+[A-Z]\\.)+)\"\n",
    "\n",
    "    match = re.search(name_regex, text_top)\n",
    "    if match:\n",
    "        return clean_extracted_name(match.group(0))\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdSmkCYhDsrP"
   },
   "source": [
    "Fallback Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3UMYSMSDdTVq"
   },
   "outputs": [],
   "source": [
    "def parse_resume_with_rules(resume_text, skill_map, nlp_model):\n",
    "    \"\"\"\n",
    "    Parses resume text using a multi-layered, rule-based approach.\n",
    "\n",
    "    Args:\n",
    "        resume_text (str): The raw text of the resume.\n",
    "        skill_map (dict): A mapping of skill variations to canonical names.\n",
    "        nlp_model: The loaded spaCy model for NER.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted information.\n",
    "    \"\"\"\n",
    "    if not resume_text:\n",
    "        return None\n",
    "\n",
    "    # --- Email Extraction (Do this first as it can help with name extraction) ---\n",
    "    email = None\n",
    "    email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', resume_text)\n",
    "    if email_match:\n",
    "        email = email_match.group(0)\n",
    "\n",
    "    # --- Name Extraction (Multi-layered approach) ---\n",
    "    name = None\n",
    "    # 1. Try NER first - it's the most intelligent\n",
    "    name = extract_name_with_ner(resume_text, nlp_model)\n",
    "\n",
    "    # 2. If NER fails, try finding the name near the email\n",
    "    if not name:\n",
    "        name = extract_name_near_email(resume_text, email)\n",
    "\n",
    "    # 3. As a last resort, use the improved top-of-document search\n",
    "    if not name:\n",
    "        name = extract_name_from_top(resume_text)\n",
    "\n",
    "    # --- Skill Extraction (Using the advanced SKILL_MAP) ---\n",
    "    found_canonical_skills = set()\n",
    "    resume_text_lower = resume_text.lower()\n",
    "    if skill_map:\n",
    "        for skill_alias, canonical_skill in skill_map.items():\n",
    "            if re.search(r'\\b' + re.escape(skill_alias) + r'\\b', resume_text_lower):\n",
    "                found_canonical_skills.add(canonical_skill)\n",
    "\n",
    "    # --- Assemble final JSON object ---\n",
    "    parsed_data = {\n",
    "        \"name\": name,\n",
    "        \"email\": email,\n",
    "        \"skills\": sorted(list(found_canonical_skills))\n",
    "    }\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "# --- FINAL EXECUTION EXAMPLE ---\n",
    "# 1. Load dependencies at the start\n",
    "# SKILL_MAP = build_skill_map_from_csv('skills.csv')\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5e9CArbD3rD"
   },
   "source": [
    "### 6) Unified Pipeline\n",
    "\n",
    "`parse_resume(file_path)` tries **Gemini** first (if available), otherwise falls back to the rules‑based parser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H0lErsX7Dd7d"
   },
   "outputs": [],
   "source": [
    "def process_resume(file_path, skill_map):\n",
    "    \"\"\"\n",
    "    Main function to process a resume file.\n",
    "    It extracts text, tries the GenAI parser, and uses a rule-based\n",
    "    fallback with an advanced skill map if the GenAI parser fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing {os.path.basename(file_path)} ---\")\n",
    "\n",
    "    text = extract_text(file_path)\n",
    "    if not text:\n",
    "        print(\"Could not extract text. Aborting.\")\n",
    "        return None, \"Error\"\n",
    "\n",
    "    print(\"Attempting to parse with GenAI...\")\n",
    "    parsed_data = parse_resume_with_genai(text)\n",
    "\n",
    "    if parsed_data is None:\n",
    "        print(\"GenAI parsing failed. Switching to rule-based fallback parser...\")\n",
    "        # Pass the loaded SKILL_MAP to the fallback function\n",
    "        parsed_data = parse_resume_with_rules(text, skill_map)\n",
    "        parser_used = \"Fallback (Rules)\"\n",
    "    else:\n",
    "        parser_used = \"Primary (GenAI)\"\n",
    "\n",
    "    print(f\"Parsing complete. Method used: {parser_used}\")\n",
    "    return parsed_data, parser_used\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq3iaWXrEydz"
   },
   "source": [
    "### 7) Final Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "id": "Z7idSoudEwT0",
    "outputId": "5e349308-a35b-43e0-e397-9617101b2559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded skills.csv with 13893 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/bc2q7h4x0xd_1v2gtlr1f7xc0000gp/T/ipykernel_96432/1537022553.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['altLabels'].fillna('', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a skill map with 27178 total variations.\n",
      "\n",
      "--- Processing sample_resume4.pdf ---\n",
      "Attempting to parse with GenAI...\n",
      "Parsing complete. Method used: Primary (GenAI)\n",
      "\n",
      "--- Final Parsed Data ---\n",
      "{\n",
      "  \"name\": \"JULIE MONROE\",\n",
      "  \"email\": \"email@email.com\",\n",
      "  \"skills\": [\n",
      "    \"Food preparation\",\n",
      "    \"Kitchen maintenance\",\n",
      "    \"Kitchen equipment operation\",\n",
      "    \"Food sanitation\",\n",
      "    \"Nutrition\",\n",
      "    \"nutrition education counseling\",\n",
      "    \"nutrition assessments\",\n",
      "    \"WIC certifications\",\n",
      "    \"nutritional status assessment\",\n",
      "    \"behavior and lifestyle changes\",\n",
      "    \"osteodystrophy management\",\n",
      "    \"anemia management\",\n",
      "    \"nutrition management\",\n",
      "    \"bone mineral status monitoring\",\n",
      "    \"TPN usage reduction\",\n",
      "    \"nutritional care\",\n",
      "    \"staff supervision\",\n",
      "    \"mass media outreach\",\n",
      "    \"program development\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 1. First, build the map\n",
    "SKILL_MAP = build_skill_map_from_csv('skills.csv')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 2. Then, process a file by soecifying its path\n",
    "## Modify the line below and provide your own file path\n",
    "sample_pdf_path = '~/sample_resume4.pdf'\n",
    "if os.path.exists(sample_pdf_path) and SKILL_MAP:\n",
    "    final_data, method = process_resume(sample_pdf_path, SKILL_MAP)\n",
    "    print(\"\\n--- Final Parsed Data ---\")\n",
    "    print(json.dumps(final_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (resume_env)",
   "language": "python",
   "name": "resume_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
